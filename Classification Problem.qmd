---
title: "Classification Problem"
author: "Xinzhu Li"
format: pdf
editor: visual
---

# Introduction

We used the Breast Cancer dataset to solve the classification problem. The analysis consists of 4 parts:

1: logistic regression;

2: decision tree;

3: random forest;

4: SVM

These 4 methods were applied to the Breast Cancer dataset to compare their differences and gain a better understanding of each approach.

## Logistic Regression

First, we’ll load the data and check if there are any missing values in the dataset. After that, we’ll transform the data into the proper format. Then, we’ll split the dataset into a training set and a test set, with a ratio of 70% for training and 30% for testing.

```{r}
library("mlbench")
data("BreastCancer")
head(BreastCancer)
names(BreastCancer)
str(BreastCancer)



df <- BreastCancer[-1]
df
sum(is.na(df))
colSums(is.na(df))
df <- na.omit(df)
df$Cl.thickness <- as.numeric(df$Cl.thickness)
df$Cell.size <- as.numeric(df$Cell.size)
df$Cell.shape <- as.numeric(df$Cell.shape)
df$Marg.adhesion <- as.numeric(df$Marg.adhesion)
df$Epith.c.size <- as.numeric(df$Epith.c.size)
df$Bare.nuclei <- as.numeric(df$Bare.nuclei)
df$Bl.cromatin <- as.numeric(df$Bl.cromatin)
df$Normal.nucleoli <- as.numeric(df$Normal.nucleoli)
df$Mitoses <- as.numeric(df$Mitoses)

set.seed(1234)
index <- sample(nrow(df), 0.7*nrow(df))
train <- df[index,]
test <- df[-index,]

```

Then, we use the nine variables to perform logistic regression for classification, where the class is the dependent variable. We use the summary function to examine the details of the fit result. Next, we use the test data for prediction and set type = 'response' to see the probability of the malignant class.

Next, we set a threshold for the probability: if the probability is greater than 0.5, we label it as ‘malignant’, and if it is less than 0.5, we label it as ‘benign’. Then, we convert these ‘benign’ and ‘malignant’ labels into factors for further classification, as we will use the table function.

```{r}
fit <- glm(Class ~ ., data = train, family = binomial())
summary(fit)


prob <- predict(fit,test,type="response")

prediction <- factor(prob>0.5, levels=c(FALSE,TRUE), labels = c("benign","malignant"))
prediction
```

Finally, we generate a table comparing the actual classification with the predicted classification, which allows us to view the final results. Based on this table, we calculate four key metrics to evaluate the model’s performance: Accuracy, Precision, Recall, and F1 Score. From these metrics, we observe that the result is generally acceptable and high.

```{r}

perf1 <- table(test$Class, prediction, dnn=c("Actual","Predicted"))
perf1


TN1 <- perf1[1, 1]  # True Negatives
TP1 <- perf1[2, 2]  # True Positives

FP1 <- perf1[1, 2]  # False Positives
FN1 <- perf1[2, 1]  # False Negatives


#  (Accuracy)
accuracy1 <- (TP1 + TN1) / sum(perf1)

# (Precision)
precision1 <- TP1 / (TP1 + FP1)

#  (Recall)
recall1 <- TP1 / (TP1 + FN1)


spcificity1 <- TN1/(TN1 + FP1)

list(
  Accuracy1 = accuracy1,
  Precision1 = precision1,
  Recall1 = recall1,
  Spcificity1 = spcificity1
)
```

## Decision Tree

Next, we move on to the Decision Tree method. We set the parameters to use information gain instead of the Gini index. By using the print and summary functions, we can examine the details of the decision tree.

```{r}
library("rpart")
dtree <- rpart(Class ~ .,data= train, method="class",parms=list(split="information"))

print(dtree)
summary(dtree)
```

In order to simplify the model, improve calculation efficiency, and enhance stability, we use the prune function to help optimize the decision tree.

By examining the complexity parameter table, we can determine the optimal pruning point for the decision tree, which shows different values ​​of cp, the number of branches in the tree (nsplit), the relative error (rel error), the cross-validation error (xerror), and its standard deviation (xstd).

The general approach is to choose the value of **CP** that minimizes the **cross-validation error (xerror)**.

In that case, we will choose the 0.01 for the CP value. After pruning the decision tree, we can use fancyRpartPlot function to visualize the decision tree and its structure clearly.

```{r}
plotcp(dtree)
dtree$cptable
dtree.pruned <- prune(dtree,cp=0.01)

library(rattle)
fancyRpartPlot(dtree.pruned, sub="Classification Tree")
```

After pruning the decision tree, we can use the test data to make predictions. Similar to logistic regression, we can use the table() function to compare the actual classifications with the predicted classifications.

```{r}
dtree.pred <- predict(dtree.pruned,test,type="class")
perf2 <- table(test$Class,dtree.pred,dnn=c("Actual","Predicted"))
perf2
```

```{r}


TN2 <- perf2[1, 1]  # True Negatives
FP2 <- perf2[1, 2]  # False Positives
FN2 <- perf2[2, 1]  # False Negatives
TP2 <- perf2[2, 2]  # True Positives

#  (Accuracy)
accuracy2 <- (TP2 + TN2) / sum(perf2)

#  (Precision)
precision2 <- TP2 / (TP2 + FP2)

#  (Recall)
recall2 <- TP2 / (TP2 + FN2)

spcificity2 <- TN2/(TN2 + FP2)

list(
  Accuracy2 = accuracy2,
  Precision2 = precision2,
  Recall2 = recall2,
  Spcificity2 =  spcificity2
)
```

## RandomForestModel

### RandomForest

Now, let’s move on to the random forest method. We use the randomForest function to build the model and the importance function to evaluate and rank the variables based on their importance.

```{r}
library(randomForest)

set.seed(1234)
fit.forest <- randomForest(Class ~ ., data=train,importance= TRUE)
fit.forest

var_importance <- fit.forest$importance
print(var_importance)
```

Use the test data

```{r}

forest.pred <- predict(fit.forest,test)
perf3 <- table(test$Class,forest.pred,dnn=c("Actual","Predicted"))
perf3 


TN3 <- perf3[1, 1]  # True Negatives
FP3 <- perf3[1, 2]  # False Positives
FN3 <- perf3[2, 1]  # False Negatives
TP3 <- perf3[2, 2]  # True Positives


accuracy3 <- (TP3 + TN3) / sum(perf3)

precision3 <- TP3 / (TP3 + FP3)


recall3 <- TP3 / (TP3 + FN3)
spcificity3 <- TN3/(TN3 + FP3)

list(
  Accuracy3 = accuracy3,
  Precision3 = precision3,
  Recall3 = recall3,
  Spcificity3 = spcificity3
)
```

### Black-box characteristic

These classification models are really important in real-world applications, as they can have a significant impact on people’s decisions. For example, if someone applies for a bank loan and gets rejected, we would want to know the reason. If the decision is based on a logistic regression model or a decision tree model, we can easily understand the exact reason from the coefficients of the logistic regression model or the decision tree plot. However, if the decision is based on a random forest model, how can we get the answer? In random forest, multiple decision trees are used, and the paths of each tree are determined by various factors. As a result, we don’t have a clear understanding of the decision path. To address this, there are methods we can use to interpret black box characteristics. One way is by applying explainable artificial intelligence (XAI) techniques to make these decisions more transparent and understandable.

For example, if we randomly generate data for Amy and use the model to calculate her probability of being malignant, and the result shows that her probability of being malignant is nearly 67%, which is greater than 50%, we want to understand how this result is influenced by the nine variables. Specifically, we want to know the contribution of each variable to the final decision. To do this, we use the explainer and plot to clearly visualize how each variable contributes to the model’s output.

From the two charts below, we can observe that the results are consistent. The most important factor is the CL thickness, as its contribution is the highest among all the variables

```{r}
library("DALEX")

Amy <- data.frame(
  Cl.thickness = 9,
  Cell.size = 3,
  Cell.shape = 1,
  Marg.adhesion =7,
  Epith.c.size = 1,
  Bare.nuclei = 3,
  Bl.cromatin = 3,
  Normal.nucleoli = 6,
  Mitoses = 3
)

predict(fit.forest,Amy,type="prob")


explainer_fr_malignant <- explain(fit.forest,data=train,y=train$Class == "malignant",
                      predcit_function = function(m,x) predict(m,x,type = "prob")[,2])

rf_pparts1  <- predict_parts(explainer_fr_malignant,new_observation = Amy,type="break_down")

plot(rf_pparts1 )


rf_pparts2  <- predict_parts(explainer_fr_malignant,new_observation = Amy,type="shap")

plot(rf_pparts2 )
```

## Support vector machines

### SVM

The results from the Support Vector Machine (SVM) model demonstrate strong performance, as evidenced by the four performance metrics.

```{r}
library(e1071)
set.seed(1234)
fit.svm <- svm(Class ~ ., data=train)
fit.svm

svm.pred <- predict(fit.svm,test)
svm.perf <- table(test$Class,svm.pred,dnn=c("Actual","Predicted"))
svm.perf 


TN <- svm.perf[1, 1]  # True Negatives
FP <- svm.perf[1, 2]  # False Positives
FN <- svm.perf[2, 1]  # False Negatives
TP <- svm.perf[2, 2]  # True Positives

accuracy <- (TP + TN) / sum(svm.perf)
precision <- TP / (TP + FP)
recall <- TP / (TP + FN)
spcificity <- TN/(TN + FP)


list(
  Accuracy = accuracy,
  Precision = precision,
  Recall = recall,
  Spcificity =spcificity
)
```

### Parameters

There are two key hyperparameters in Support Vector Machines (SVM): gamma and cost. By adjusting these parameters, we can enhance the model’s performance. From the code results below, the optimal parameter values are found to be gamma = 0.01 and cost = 1. After updating the default parameters with these values and re-running the model, we observe that the results improve marginally, with one additional benign instance being accurately predicted. While the overall performance remains largely unchanged, it’s important to note that tuning parameters in SVM typically leads to improvements in model performance.

```{r}

#SVM调优
set.seed(1234)
tuned <- tune.svm(Class~.,data=train,gamma=10^(-6:1),cost=10^(-10:10))
tuned 

fit.svm1 <- svm(Class~., data=train,gamma=0.01,cost=1)
svm.pred1 <- predict(fit.svm1,na.omit(test))
svm.perf1 <- table(na.omit(test)$Class,svm.pred1,dnn=c("Actual","Predicted"))
svm.perf1 

```

# Summary

Among the four models—Logistic Regression, Decision Tree, Random Forest, and Support Vector Machine—the results from all models are generally acceptable, with good performance across the board. However, in practice, the decision to choose the best model depends on the specific context. In this case, while the Support Vector Machine (SVM) appears to offer slightly better performance, the differences among the models are minimal.

From a cancer diagnosis perspective, recall is a critical metric, as it reflects the model’s ability to accurately identify malignant cases. Therefore, this metric should be prioritized when selecting the most suitable model. Among the four models, the Support Vector Machine (SVM) demonstrates the highest recall rate, making it the preferred choice for this task.

Logistic regression is a simple and interpretable model that provides probabilities, making it easy to understand and useful for decision-making. It works well with linear data but struggles with nonlinear relationships, limiting its performance in more complex scenarios. Decision trees, on the other hand, can handle nonlinear data and provide a clear, interpretable structure through a tree diagram. They are capable of capturing more complex patterns than logistic regression. Random forests, an ensemble of multiple decision trees, improve upon decision trees by reducing overfitting and improving accuracy. They excel at handling missing data and complex nonlinear relationships, making them more robust and accurate than individual decision trees, but at the cost of interpretability.The Support Vector Machine (SVM) is a widely used and popular method, known for its versatility across various applications. It is particularly useful in scenarios where the number of variables exceeds the number of observations, a common challenge in the pharmaceutical industry. Like Random Forest, SVM’s classification principle can be difficult to interpret, as it functions as a black-box model. Additionally, when dealing with large datasets, SVM may not perform as well as Random Forest. However, once a successful model is developed, it is highly effective for classifying new observations.

```{r}
list(
  Accuracy1 = accuracy1,
  Precision1 = precision1,
  Recall1 = recall1,
  spcificity1 = spcificity1
)

list(
  Accuracy2 = accuracy2,
  Precision2 = precision2,
  Recall2 = recall2,
  Spcificity2 = spcificity2
)

list(
  Accuracy3 = accuracy3,
  Precision3 = precision3,
  Recall3 = recall3,
  Spcificity3 = spcificity3
)

list(
  Accuracy4 = accuracy,
  Precision4 = precision,
  Recall4 = recall,
  Spcificity4 = spcificity
)
```
